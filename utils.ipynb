{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec976b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting onnx model version / Preprocess input images required for runtime validation on the development board / Local verification of onnx running on the development board\n"
    "import onnx\n",
    "from onnx import version_converter, helper\n",
    "\n",
    "# import onnxruntime\n",
    "# help(onnx)\n",
    "\n",
    "# Preprocessing: load the model to be converted.\n",
    "model_path = 'model_99.onnx'\n",
    "original_model = onnx.load(model_path)\n",
    "original_model.opset_import[0].version = 11\n",
    "original_model.ir_version = 6\n",
    "\n",
    "onnx.save(original_model, \"new.onnx\")\n",
    "# print(original_model.Version)\n",
    "\n",
    "\n",
    "# print('The model before conversion:\\n{}'.format(original_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "954338c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "output_name",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion-ferplus-8.onnx\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     11\u001b[0m original_model \u001b[38;5;241m=\u001b[39m onnx\u001b[38;5;241m.\u001b[39mload(model_path)\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43moriginal_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_name\u001b[49m)\n\u001b[0;32m     14\u001b[0m net \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mreadNetFromONNX(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memotion-ferplus-8.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m net\u001b[38;5;241m.\u001b[39msetInput(img)\n",
      "\u001b[1;31mAttributeError\u001b[0m: output_name"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import onnx\n",
    "\n",
    "img = cv.imread(\"fer0032220.png\",0)\n",
    "img_t = cv.resize(img,(64,64))\n",
    "print(img_t.shape)\n",
    "#img_t = img_t[np.newaxis,:]   #扩展一个新维度\n",
    "\n",
    "model_path = 'emotion-ferplus-8.onnx'\n",
    "original_model = onnx.load(model_path)\n",
    "print(original_model.output_name)\n",
    "\n",
    "net = cv.dnn.readNetFromONNX(\"emotion-ferplus-8.onnx\")\n",
    "net.setInput(img)\n",
    "outs = net.forward()\n",
    "print(outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8043a016",
   "metadata": {},
   "source": [
    "# data_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fbdd1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write:F:\\github\\BPUcode\\calibration_data\\1000268201_693b08cb0e.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1001773457_577c3a7d70.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1002674143_1b742ab4b8.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1003163366_44323f5815.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1007129816_e794419615.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1007320043_627395c3d8.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1009434119_febe49276a.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1012212859_01547e3f17.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1015118661_980735411b.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1015584366_dfcec3c85a.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\101654506_8eb26cfb60.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\101669240_b2d3e7f17b.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1016887272_03199f49c4.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1019077836_6fc9b15408.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1019604187_d087bf9a5f.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1020651753_06077ec457.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1022454332_6af2c1449a.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1022454428_b6b660a67b.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1022975728_75515238d8.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\102351840_323e3de834.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1024138940_f1fefbdce1.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\102455176_5f8ead62d5.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1026685415_0431cbf574.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1028205764_7e8df9a2ea.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1030985833_b0902ea560.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\103106960_e8a41d64f8.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\103195344_5d2dc613a3.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\103205630_682ca7285b.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1032122270_ea6f0beedb.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1032460886_4a598ed535.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1034276567_49bb87c51c.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\104136873_5b5d41be75.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1042020065_fb3d3ba5ba.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1042590306_95dea0916c.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1045521051_108ebc19be.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1048710776_bb5b0a5c7c.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1052358063_eae6744153.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\105342180_4d4a40b47f.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1053804096_ad278b25f1.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1055623002_8195a43714.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1055753357_4fa3d8d693.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1056249424_ef2a2e041c.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1056338697_4f7d7ce270.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1056359656_662cee0814.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1056873310_49c665eb22.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1057089366_ca83da0877.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1057210460_09c6f4c6c1.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1057251835_6ded4ada9c.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\106490881_5a2dd9b7bd.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\106514190_bae200f463.jpg\n",
      "write:F:\\github\\BPUcode\\calibration_data\\1067180831_a59dc64344.jpg\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "# prepare_calibration_data.py\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "src_root = 'F:\\dataset\\Flicker8k_Dataset'\n",
    "cal_img_num = 50  # 想要的图像个数\n",
    "dst_root = 'F:\\github\\BPUcode\\calibration_data'\n",
    "\n",
    "\n",
    "## 1. 从原始图像文件夹中获取100个图像作为校准数据\n",
    "num_count = 0\n",
    "img_names = []\n",
    "for src_name in sorted(os.listdir(src_root)):\n",
    "    if num_count > cal_img_num:\n",
    "        break\n",
    "    img_names.append(src_name)\n",
    "    num_count += 1\n",
    "\n",
    "# 检查目标文件夹是否存在，如果不存在就创建\n",
    "if not os.path.exists(dst_root):\n",
    "    os.system('mkdir {0}'.format(dst_root))\n",
    "\n",
    "## 2 为每个图像转换\n",
    "# 参考了OE中/open_explorer/ddk/samples/ai_toolchain/horizon_model_convert_sample/01_common/python/data/下的相关代码\n",
    "# 转换代码写的很棒，很智能，考虑它并不是官方python包，所以我打算换一种写法\n",
    "\n",
    "## 2.1 定义图像缩放函数，返回为np.float32\n",
    "# 图像缩放为目标尺寸(W, H)\n",
    "# 值得注意的是，缩放时候，长宽等比例缩放，空白的区域填充颜色为pad_value, 默认127\n",
    "def imequalresize(img, target_size, pad_value=127.):\n",
    "    target_w, target_h = target_size\n",
    "    image_h, image_w = img.shape[:2]\n",
    "    img_channel = 3 if len(img.shape) > 2 else 1\n",
    "\n",
    "    # 确定缩放尺度，确定最终目标尺寸\n",
    "    scale = min(target_w * 1.0 / image_w, target_h * 1.0 / image_h)\n",
    "    new_h, new_w = int(scale * image_h), int(scale * image_w)\n",
    "\n",
    "    resize_image = cv2.resize(img, (new_w, new_h))\n",
    "\n",
    "    # 准备待返回图像\n",
    "    pad_image = np.full(shape=[target_h, target_w, img_channel], fill_value=pad_value)\n",
    "\n",
    "    # 将图像resize_image放置在pad_image的中间\n",
    "    dw, dh = (target_w - new_w) // 2, (target_h - new_h) // 2\n",
    "    pad_image[dh:new_h + dh, dw:new_w + dw, :] = resize_image\n",
    "\n",
    "    return pad_image\n",
    "\n",
    "## 2.2 开始转换\n",
    "for each_imgname in img_names:\n",
    "    img_path = os.path.join(src_root, each_imgname)\n",
    "\n",
    "    img = cv2.imread(img_path,0)  # BRG, HWC\n",
    "    #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # RGB, HWC\n",
    "    #img = imequalresize(img, (64, 64))\n",
    "    #img = np.transpose(img, (2, 0, 1))  # RGB, CHW\n",
    "    img = cv2.resize(img, (64, 64))\n",
    "    # 将图像保存到目标文件夹下\n",
    "    dst_path = os.path.join(dst_root, each_imgname)\n",
    "    print(\"write:%s\" % dst_path)\n",
    "    # 图像加载默认就是uint8，但是不加这个astype的话转换模型就会出错\n",
    "    # 转换模型时候，加载进来的数据竟然是float64，不清楚内部是怎么加载的。\n",
    "    #img.astype(np.uint8).tofile(dst_path) \n",
    "    img.tofile(dst_path) \n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd2a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from horizon_tc_ui import HB_ONNXRuntime\n",
    "import copy\n",
    "\n",
    "# img_path 图像完整路径\n",
    "img_path = 'F:\\github\\BPUcode\\calibration_data'\n",
    "# model_path 量化模型完整路径\n",
    "model_path = 'F:\\github\\BPUcode\\model_output\\emotion_ferplus_quantized_model.onnx'\n",
    "\n",
    "# 1. 加载模型，获取所需输出HW\n",
    "sess = HB_ONNXRuntime(model_file=model_path)\n",
    "sess.set_dim_param(0, 0, '?')\n",
    "model_h, model_w = sess.get_hw()\n",
    "\n",
    "# 2 加载图像，根据前面yaml，量化后的模型以BGR NHWC形式输入\n",
    "imgOri = cv2.imread(img_path)\n",
    "img = cv2.resize(imgOri, (model_w, model_h))\n",
    "\n",
    "# 3 模型推理\n",
    "input_name = sess.input_names[0]\n",
    "output_name = sess.output_names\n",
    "output = sess.run(output_name, {input_name: np.array([img])}, input_offset=128)\n",
    "print(output_name)\n",
    "print(output[0].shape)\n",
    "# ['net_output']\n",
    "# (1, 22, 46, 46)\n",
    "\n",
    "# 4 检测结果后处理\n",
    "# 绘制关键点\n",
    "nPoints = 22\n",
    "threshold = 0.1\n",
    "POSE_PAIRS = [[0, 1], [1, 2], [2, 3], [3, 4], [0, 5], [5, 6], [6, 7], [7, 8], [0, 9], [9, 10], [10, 11], [11, 12],\n",
    "              [0, 13], [13, 14], [14, 15], [15, 16], [0, 17], [17, 18], [18, 19], [19, 20]]\n",
    "\n",
    "imgh, imgw = imgOri.shape[:2]\n",
    "points = []\n",
    "imgkp = copy.deepcopy(imgOri)\n",
    "for i in range(nPoints):\n",
    "    probMap = output[0][0, i, :, :]\n",
    "    probMap = cv2.resize(probMap, (imgw, imgh))\n",
    "    minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
    "\n",
    "    if prob > threshold:\n",
    "        cv2.circle(imgkp, (int(point[0]), int(point[1])), 8, (0, 255, 255), thickness=-1, lineType=cv2.FILLED)\n",
    "        cv2.putText(imgkp, \"{}\".format(i), (int(point[0]), int(point[1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2,\n",
    "                    lineType=cv2.LINE_AA)\n",
    "        points.append((int(point[0]), int(point[1])))\n",
    "    else:\n",
    "        points.append(None)\n",
    "\n",
    "# 绘制骨架\n",
    "imgskeleton = copy.deepcopy(imgOri)\n",
    "for pair in POSE_PAIRS:\n",
    "    partA = pair[0]\n",
    "    partB = pair[1]\n",
    "    if points[partA] and points[partB]:\n",
    "        cv2.line(imgskeleton, points[partA], points[partB], (0, 255, 255), 2)\n",
    "        cv2.circle(imgskeleton, points[partA], 8, (0, 0, 255), thickness=-1, lineType=cv2.FILLED)\n",
    "        cv2.circle(imgskeleton, points[partB], 8, (0, 0, 255), thickness=-1, lineType=cv2.FILLED)\n",
    "# 保存关键点和骨架图\n",
    "cv2.imwrite('handkeypoint.png', imgkp)\n",
    "cv2.imwrite('imgskeleton.png', imgskeleton)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
